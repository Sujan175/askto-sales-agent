#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""askto - Pipecat Voice Agent

This bot uses a cascade pipeline: Speech-to-Text → LLM → Text-to-Speech

Generated by Pipecat CLI

Required AI services:
- Sarvam (Speech-to-Text)
- Openrouter (LLM)
- Sarvam (Text-to-Speech)

Run the bot using::

    uv run bot.py
"""

from pipecat.services.sarvam.stt import SarvamSTTService
from pipecat.services.sarvam.tts import SarvamTTSService

from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.frames.frames import EndFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.services.openrouter.llm import OpenRouterLLMService
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.frames.frames import LLMRunFrame, MetricsFrame, OutputTransportMessageFrame, EndTaskFrame
from pipecat.metrics.metrics import LLMUsageMetricsData
from pipecat.processors.frame_processor import FrameProcessor, FrameDirection
from pipecatcloud import PipecatSessionArguments, SmallWebRTCSessionManager
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from dotenv import load_dotenv
from pipecat.runner.types import RunnerArguments
from pipecat.pipeline.runner import PipelineRunner
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.transports.base_transport import BaseTransport
from loguru import logger
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport
import os
import uuid
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.runner.types import SmallWebRTCRunnerArguments
from pipecat.transports.daily.transport import DailyTransport, DailyParams
from pipecat.transports.base_transport import TransportParams
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.runner.types import DailyRunnerArguments
from pipecat.services.openai.base_llm import BaseOpenAILLMService


load_dotenv(override=True)

# Create a global session manager instance for SmallWebRTC
session_manager = SmallWebRTCSessionManager(timeout_seconds=120)

# Store session-specific configuration
# We'll use a simple approach: store the most recent systemPrompt and sessionType
# This works for single-user scenarios or when sessions are created sequentially
session_configs = {}
latest_system_prompt = None
latest_session_type = None  # For LangGraph: 'discovery', 'pitch', or 'objection'

MAX_TOKENS = int(os.getenv("MAX_TOKENS") or 10000)
MAX_COINS = int(os.getenv("MAX_COINS") or 10)

TOKENS_PER_COIN = MAX_TOKENS / MAX_COINS

logger.info(f"Configuration loaded: MAX_TOKENS={MAX_TOKENS}, MAX_COINS={MAX_COINS}, TOKENS_PER_COIN={TOKENS_PER_COIN}")


class TokenUsageReporter(FrameProcessor):
    """Reports token usage to frontend and converts to quota/coins.
    
    Sends real-time updates via rtvi.send_server_message().
    Coins are calculated based on MAX_TOKENS/MAX_COINS ratio.
    Example: 10,000 tokens / 10 coins = 1000 tokens per coin
    """
    def __init__(self, rtvi_processor, max_tokens=None, max_coins=None):
        super().__init__()
        self.rtvi = rtvi_processor
        self.max_tokens = max_tokens or MAX_TOKENS
        self.max_coins = max_coins or MAX_COINS
        self.tokens_per_coin = self.max_tokens / self.max_coins
        self.total_tokens_used = 0
        logger.info(f"TokenUsageReporter initialized: {self.max_tokens} tokens, {self.max_coins} coins ({self.tokens_per_coin} tokens/coin)")

    def set_state(self, tokens_used):
        self.total_tokens_used = tokens_used
        logger.info(f"TokenUsageReporter state restored: {self.total_tokens_used} tokens used")

    async def process_frame(self, frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, MetricsFrame):
            for d in frame.data:
                if isinstance(d, LLMUsageMetricsData):
                    usage = d.value
                    used = usage.prompt_tokens + usage.completion_tokens

                    self.total_tokens_used += used
                    tokens_remaining = max(self.max_tokens - self.total_tokens_used, 0)

                    coins_used = self.total_tokens_used / self.tokens_per_coin
                    coins_remaining = max(self.max_coins - coins_used, 0)

                    # Check if token limit exceeded
                    if self.total_tokens_used > self.max_tokens:
                        error_msg = {
                            "type": "token_limit_exceeded",
                            "data": {
                                "message": "You have exceeded the token limit",
                                "tokens_used": self.total_tokens_used,
                                "max_tokens": self.max_tokens,
                                "coins_used": round(coins_used, 4),
                                "max_coins": self.max_coins,
                            }
                        }
                        logger.warning(f"Token limit exceeded! Sending error to client: {error_msg}")
                        await self.rtvi.send_server_message(error_msg)

                        await self.push_frame(frame, direction)
                        await self.push_frame(EndTaskFrame(), FrameDirection.UPSTREAM)
                        return

                    msg = {
                        "type": "token_usage",
                        "data": {
                            "tokens_used": self.total_tokens_used,
                            "max_tokens": self.max_tokens,
                            "tokens_remaining": tokens_remaining,
                            "coins_used": round(coins_used, 4),
                            "coins_remaining": round(coins_remaining, 4),
                            "max_coins": self.max_coins,
                            "tokens_per_coin": self.tokens_per_coin,
                        }
                    }

                    logger.info(f"Sending token usage to client: {msg}")

                    await self.rtvi.send_server_message(msg)

        await self.push_frame(frame, direction)


def extract_system_prompt(runner_args: RunnerArguments) -> str | None:
    """Extract system prompt from runner_args or global storage."""
    global latest_system_prompt
    
    logger.debug(f"Attempting to extract system prompt from {type(runner_args).__name__}")
    
    # First check if we have a stored latest_system_prompt
    if latest_system_prompt:
        logger.info(f"✓ Found system prompt in latest_system_prompt (length: {len(latest_system_prompt)})")
        result = latest_system_prompt
        latest_system_prompt = None  # Clear after use
        return result
    
    # For SmallWebRTCRunnerArguments
    if isinstance(runner_args, SmallWebRTCRunnerArguments):
        # Check body
        if hasattr(runner_args, 'body'):
            body = getattr(runner_args, 'body', None)
            logger.debug(f"Found body attribute: {type(body)} = {body}")
            if isinstance(body, dict) and 'systemPrompt' in body:
                system_prompt = body['systemPrompt']
                logger.info(f"✓ Found system prompt in SmallWebRTCRunnerArguments.body (length: {len(system_prompt)})")
                return system_prompt
        
        # Check direct attribute
        if hasattr(runner_args, 'systemPrompt'):
            system_prompt = getattr(runner_args, 'systemPrompt', None)
            if system_prompt:
                logger.info(f"✓ Found system prompt as direct attribute (length: {len(system_prompt)})")
                return system_prompt
        
        # Check config
        if hasattr(runner_args, 'config'):
            config = getattr(runner_args, 'config', None)
            if isinstance(config, dict) and 'systemPrompt' in config:
                system_prompt = config['systemPrompt']
                logger.info(f"✓ Found system prompt in SmallWebRTCRunnerArguments.config (length: {len(system_prompt)})")
                return system_prompt
        
        # Inspect __dict__
        if hasattr(runner_args, '__dict__'):
            args_dict = runner_args.__dict__
            logger.debug(f"SmallWebRTCRunnerArguments.__dict__ keys: {list(args_dict.keys())}")
            for key, value in args_dict.items():
                if isinstance(value, dict) and 'systemPrompt' in value:
                    system_prompt = value['systemPrompt']
                    logger.info(f"✓ Found system prompt in {key} (length: {len(system_prompt)})")
                    return system_prompt
    
    # For PipecatSessionArguments
    if isinstance(runner_args, PipecatSessionArguments):
        if hasattr(runner_args, 'config') and isinstance(runner_args.config, dict):
            system_prompt = runner_args.config.get('systemPrompt')
            if system_prompt:
                logger.info(f"✓ Found system prompt in PipecatSessionArguments.config (length: {len(system_prompt)})")
                # Store for later use
                latest_system_prompt = system_prompt
                return system_prompt
    
    # For DailyRunnerArguments
    if isinstance(runner_args, DailyRunnerArguments):
        if hasattr(runner_args, 'body'):
            body = getattr(runner_args, 'body', None)
            if isinstance(body, dict) and 'systemPrompt' in body:
                system_prompt = body['systemPrompt']
                logger.info(f"✓ Found system prompt in DailyRunnerArguments.body (length: {len(system_prompt)})")
                return system_prompt
        
        if hasattr(runner_args, 'config') and isinstance(runner_args.config, dict):
            system_prompt = runner_args.config.get('systemPrompt')
            if system_prompt:
                logger.info(f"✓ Found system prompt in DailyRunnerArguments.config (length: {len(system_prompt)})")
                return system_prompt
    
    logger.warning("✗ No custom system prompt found, will use default")
    return None


def extract_session_type(runner_args: RunnerArguments) -> str | None:
    """Extract session type from runner_args or global storage.
    
    Session types: 'discovery', 'pitch', 'objection'
    """
    global latest_session_type
    
    # First check if we have a stored latest_session_type
    if latest_session_type:
        logger.info(f"✓ Found session type in latest_session_type: {latest_session_type}")
        result = latest_session_type
        latest_session_type = None  # Clear after use
        return result
    
    # For SmallWebRTCRunnerArguments
    if isinstance(runner_args, SmallWebRTCRunnerArguments):
        if hasattr(runner_args, 'body'):
            body = getattr(runner_args, 'body', None)
            if isinstance(body, dict) and 'sessionType' in body:
                session_type = body['sessionType']
                logger.info(f"✓ Found session type in SmallWebRTCRunnerArguments.body: {session_type}")
                return session_type
    
    # For DailyRunnerArguments
    if isinstance(runner_args, DailyRunnerArguments):
        if hasattr(runner_args, 'body'):
            body = getattr(runner_args, 'body', None)
            if isinstance(body, dict) and 'sessionType' in body:
                session_type = body['sessionType']
                logger.info(f"✓ Found session type in DailyRunnerArguments.body: {session_type}")
                return session_type
    
    logger.info("✗ No session type found, will default to discovery")
    return None


async def run_bot(
    transport: BaseTransport, 
    system_prompt: str = None,
    session_type: str = None,
    use_langgraph: bool = None,
):
    """Main bot logic.
    
    Args:
        transport: The Pipecat transport (Daily or SmallWebRTC)
        system_prompt: Optional system prompt (used for legacy mode)
        session_type: Sales session type: 'discovery', 'pitch', or 'objection'
        use_langgraph: Whether to use LangGraph agent (defaults to True if session_type is provided)
    """
    logger.info("Starting bot")
    
    # Determine if we should use LangGraph
    if use_langgraph is None:
        use_langgraph = session_type in ('discovery', 'pitch', 'objection')
    
    # Default session type for LangGraph mode
    if use_langgraph and not session_type:
        session_type = 'discovery'
    
    logger.info(f"Bot mode: {'LangGraph' if use_langgraph else 'Legacy'}, Session type: {session_type}")
    
    # Use provided prompt or default (for legacy mode)
    if not system_prompt:
        system_prompt = "You are a helpful and friendly interviewer. You help candidates stay cool and composed during interviews, but you give answers in short form."
    
    if not use_langgraph:
        logger.info(f"Using system prompt: {system_prompt[:100]}..." if len(system_prompt) > 100 else f"Using system prompt: {system_prompt}")

    # Speech-to-Text service
    stt = SarvamSTTService(
        api_key=os.getenv("SARVAM_API_KEY"),
        model=os.getenv("SARVAM_STT_MODEL", "saarika:v2.5")
    )

    # Text-to-Speech service
    tts = SarvamTTSService(
        api_key=os.getenv("SARVAM_API_KEY"),
        model=os.getenv("SARVAM_TTS_MODEL", "bulbul:v2"),
        voice_id=os.getenv("SARVAM_VOICE_ID")
    )

    # LLM service - use LangGraph or legacy OpenRouter
    if use_langgraph:
        from agent import LangGraphLLMService
        
        llm = LangGraphLLMService(
            session_type=session_type,
            redis_url=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
            database_url=os.getenv("DATABASE_URL", "postgresql+asyncpg://askto:askto_secret@localhost:5433/askto_memory"),
        )
        logger.info(f"Using LangGraph LLM service with session type: {session_type}")
        
        # For LangGraph, we don't need the traditional context aggregator
        # The agent manages its own context
        messages = []
        context = LLMContext(messages)
        context_aggregator = LLMContextAggregatorPair(context)
    else:
        # Legacy OpenRouter mode
        llm = OpenRouterLLMService(
            model=os.getenv("OPENROUTER_MODEL"),
            api_key=os.getenv("OPENROUTER_API_KEY"),
            params=BaseOpenAILLMService.InputParams(
                max_completion_tokens=500,
            )
        )
        
        messages = [
            {
                "role": "system",
                "content": system_prompt,
            },
        ]
        
        context = LLMContext(messages)
        context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor()

    token_reporter = TokenUsageReporter(rtvi)

    # Pipeline - assembled from reusable components
    pipeline = Pipeline([
        transport.input(),
        rtvi,
        stt,
        context_aggregator.user(),
        llm,
        tts,
        token_reporter,
        transport.output(),
        context_aggregator.assistant(),
    ])

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[
            RTVIObserver(rtvi),
        ],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()

        session_id = str(uuid.uuid4())
        await rtvi.send_server_message({
            "type": "session_init",
            "data": {
                "session_id": session_id
            }
        })

        await rtvi.send_server_message({
            "type": "token_usage",
            "data": {
                "tokens_used": 0,
                "max_tokens": token_reporter.max_tokens,
                "tokens_remaining": token_reporter.max_tokens,
                "coins_used": 0.0,
                "coins_remaining": float(token_reporter.max_coins),
                "max_coins": token_reporter.max_coins,
                "tokens_per_coin": token_reporter.tokens_per_coin,
            }
        })

        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_app_message")
    async def on_app_message(transport, message, sender):
        if message.get("type") == "action":
            data = message.get("data", {})
            if data.get("action") == "session_resume":
                # Forward this message to RTVI for processing
                await rtvi.on_client_message(message)

    @rtvi.event_handler("on_client_message")
    async def on_client_message(rtvi, message):
        if message.type == "session_resume":
            session_id = message.data.get("session_id")
            client_tokens_used = message.data.get("tokens_used")

            if session_id and client_tokens_used is not None:
                logger.info(f"Resuming session {session_id} with {client_tokens_used} tokens")
                token_reporter.set_state(int(client_tokens_used))

                tokens_remaining = max(token_reporter.max_tokens - token_reporter.total_tokens_used, 0)
                coins_used = token_reporter.total_tokens_used / token_reporter.tokens_per_coin
                coins_remaining = max(token_reporter.max_coins - coins_used, 0)

                await rtvi.send_server_message({
                    "type": "token_usage",
                    "data": {
                        "tokens_used": token_reporter.total_tokens_used,
                        "max_tokens": token_reporter.max_tokens,
                        "tokens_remaining": tokens_remaining,
                        "coins_used": round(coins_used, 4),
                        "coins_remaining": round(coins_remaining, 4),
                        "max_coins": token_reporter.max_coins,
                        "tokens_per_coin": token_reporter.tokens_per_coin,
                    }
                })

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        quota_used = token_reporter.total_tokens_used / token_reporter.tokens_per_coin
        logger.info(
            f"Session ended - Total coins used: {quota_used:.4f}, "
            f"Total tokens: {token_reporter.total_tokens_used}"
        )
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""
    global latest_system_prompt
    
    # Print payload received
    # print("=" * 80)
    # print("PAYLOAD RECEIVED")
    # print("=" * 80)
    # print(f"Type: {type(runner_args).__name__}")
    # print(f"Full object: {runner_args}")
    # if hasattr(runner_args, '__dict__'):
    #     print(f"Attributes: {runner_args.__dict__}")
    # if hasattr(runner_args, 'body'):
    #     print(f"Body: {getattr(runner_args, 'body', None)}")
    # if hasattr(runner_args, 'config'):
    #     print(f"Config: {getattr(runner_args, 'config', None)}")
    # print(f"Latest system prompt available: {latest_system_prompt is not None}")
    # print("=" * 80)
    
    # Handle Pipecat Cloud initialization for SmallWebRTC
    if isinstance(runner_args, PipecatSessionArguments):
        logger.info("Starting the bot, waiting for webrtc_connection from Pipecat Cloud")
        
        # Store system prompt if available
        if hasattr(runner_args, 'config') and isinstance(runner_args.config, dict):
            system_prompt = runner_args.config.get('systemPrompt')
            if system_prompt:
                logger.info(f"✓ Storing custom system prompt globally (length: {len(system_prompt)})")
                latest_system_prompt = system_prompt
        
        try:
            await session_manager.wait_for_webrtc()
        except TimeoutError as e:
            logger.error(f"Timeout waiting for WebRTC connection: {e}")
            raise
        return

    # Handle SmallWebRTC connection established
    if isinstance(runner_args, SmallWebRTCRunnerArguments):
        logger.info("Received webrtc_connection from Pipecat Cloud")
        session_manager.cancel_timeout()

    # Extract system prompt
    system_prompt = extract_system_prompt(runner_args)

    transport = None

    match runner_args:
        case DailyRunnerArguments():
            transport = DailyTransport(
                runner_args.room_url,
                runner_args.token,
                "Pipecat Bot",
                params=DailyParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case SmallWebRTCRunnerArguments():
            webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection

            transport = SmallWebRTCTransport(
                webrtc_connection=webrtc_connection,
                params=TransportParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return
    
    # Extract session type for LangGraph
    session_type = extract_session_type(runner_args)
    
    try:
        await run_bot(
            transport, 
            system_prompt=system_prompt,
            session_type=session_type,
        )
    finally:
        if isinstance(runner_args, SmallWebRTCRunnerArguments):
            logger.info("Cleaning up SmallWebRTC resources")
            session_manager.complete_session()


def setup_interceptor():
    """Patch _create_server_app to add middleware and config endpoint."""
    import pipecat.runner.run as pipecat_run
    from starlette.middleware.base import BaseHTTPMiddleware
    from starlette.requests import Request as StarletteRequest
    from starlette.responses import JSONResponse
    from starlette.routing import Route
    import json
    
    # Define the middleware
    class SystemPromptInterceptor(BaseHTTPMiddleware):
        async def dispatch(self, request: StarletteRequest, call_next):
            global latest_system_prompt, latest_session_type
            
            # Check if this is a POST request to /start (or contains /start in path)
            if request.method == "POST" and "/start" in request.url.path:
                try:
                    # Read the body
                    body_bytes = await request.body()
                    
                    if body_bytes:
                        body = json.loads(body_bytes)
                        
                        # Extract and store systemPrompt if present
                        if 'systemPrompt' in body:
                            latest_system_prompt = body['systemPrompt']
                            logger.info(f"✓ Intercepted and stored systemPrompt (length: {len(latest_system_prompt)})")
                        
                        # Extract and store sessionType if present (for LangGraph)
                        if 'sessionType' in body:
                            latest_session_type = body['sessionType']
                            logger.info(f"✓ Intercepted and stored sessionType: {latest_session_type}")
                    
                    # Recreate the request with the body
                    async def receive():
                        return {"type": "http.request", "body": body_bytes}
                    
                    request = StarletteRequest(request.scope, receive)
                    
                except Exception as e:
                    logger.error(f"Error intercepting /start request: {e}")
            
            # Continue with the request
            response = await call_next(request)
            return response
    
    async def config_endpoint(request):
        """Return token configuration."""
        return JSONResponse({
            "max_tokens": MAX_TOKENS,
            "max_coins": MAX_COINS,
            "tokens_per_coin": TOKENS_PER_COIN,
            "voice_id": os.getenv("SARVAM_VOICE_ID")
        })

    # Patch the function
    if hasattr(pipecat_run, '_create_server_app'):
        original_create_app = pipecat_run._create_server_app
        
        def patched_create_app(*args, **kwargs):
            app = original_create_app(*args, **kwargs)
            app.add_middleware(SystemPromptInterceptor)
            
            app.routes.append(Route("/config", config_endpoint, methods=["GET"]))
            
            logger.info("✓ Injected SystemPromptInterceptor middleware")
            logger.info("✓ Added /config endpoint")
            return app
            
        pipecat_run._create_server_app = patched_create_app
        logger.info("✓ Patched _create_server_app")
    else:
        logger.warning("Could not find _create_server_app to patch")


if __name__ == "__main__":
    setup_interceptor()
    from pipecat.runner.run import main
    main()